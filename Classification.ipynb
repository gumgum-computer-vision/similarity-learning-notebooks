{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification as Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, Conv2D, Conv2DTranspose, merge\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model, Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras import initializers\n",
    "from keras import constraints\n",
    "from keras import losses\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import random as rand\n",
    "from scipy.spatial.distance import pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "input_size = (28,28,1)\n",
    "num_classes = 10\n",
    "\n",
    "# Load Data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape((x_train.shape[0],) + input_size)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape((x_test.shape[0],) + input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_extractor():\n",
    "    extractor = Sequential()\n",
    "    extractor.add(Conv2D(32, (3,3), input_shape=(28,28,1), activation='relu', strides=(2,2), padding='same'))\n",
    "    extractor.add(Conv2D(64, (3,3), activation='relu', strides=(2,2), padding='same'))\n",
    "    extractor.add(Conv2D(128, (3,3), activation='relu', strides=(2,2), padding='valid'))\n",
    "    extractor.add(Conv2D(128, (3,3), activation='relu', strides=(1,1), padding='same', name='conv4'))\n",
    "    extractor.add(Flatten())\n",
    "    extractor.add(Dense(256, activation='relu'))\n",
    "    extractor.add(Dense(latent_dim, activation=None))\n",
    "    \n",
    "    return extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Cluster Loss ([paper](http://ydwen.github.io/papers/WenECCV16.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Network Definition\n",
    "\n",
    "input_layer = Input(input_size)\n",
    "base_net = get_feature_extractor()\n",
    "feature_vec = base_net(input_layer)\n",
    "softmax_layer = Dense(num_classes, use_bias=False, activation='softmax')\n",
    "prediction = softmax_layer(feature_vec)\n",
    "\n",
    "# A Tensorflow variable for the class centers that we can update per iteration of training:\n",
    "class_centers = tf.Variable(tf.random_normal([num_classes,2], stddev=0.3))\n",
    "\n",
    "# Here we define a loss that combines cross-entropy with the class-center distances.\n",
    "# The lambda and alpha parameters are taken as arguments\n",
    "def cluster_loss(labels, x, lam=0.07, alpha=0.5):\n",
    "    # x is the feature vector before the softmax\n",
    "    global class_centers\n",
    "    \n",
    "    # cross-entropy:\n",
    "    xent_loss = losses.categorical_crossentropy(labels, x)\n",
    "    \n",
    "    # Compute the distances from the current class-centers to add to loss\n",
    "    classes = tf.argmax(labels, axis=1)\n",
    "    c = tf.gather( class_centers, classes )\n",
    "    c_dist = K.sum(K.square(feature_vec-c), axis=1)\n",
    "    c_loss = 0.5 * tf.reduce_mean(c_dist)\n",
    "    \n",
    "    # How many samples per class in this batch:\n",
    "    class_nums = K.sum(labels, axis=0)\n",
    "    \n",
    "    # compute the differences for the class center update\n",
    "    diffs = []\n",
    "    for i in range(0, num_classes):\n",
    "        diff = tf.cast(tf.equal(tf.cast(classes, tf.int32), tf.constant(i)) ,tf.float32) * (c-feature_vec)\n",
    "        diff = K.sum(diff, axis=0, keepdims=True)\n",
    "        diff = diff / (tf.gather(class_nums, i)+1)\n",
    "        diffs.append(diff)\n",
    "    diffs = tf.concat(diffs, axis=0)\n",
    "    \n",
    "    # Update:\n",
    "    class_centers = class_centers - (alpha*diffs)\n",
    "    \n",
    "    return xent_loss + lam*c_loss\n",
    "\n",
    "\n",
    "model = Model(input_layer, prediction)\n",
    "model.compile(loss=cluster_loss, optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, to_categorical(y_train), epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_acc = (model.predict(x_train).argmax(axis=1) == y_train).sum() / float(y_train.shape[0])\n",
    "te_acc = (model.predict(x_test).argmax(axis=1) == y_test).sum() / float(y_test.shape[0])\n",
    "\n",
    "print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = Model(input_layer, feature_vec)\n",
    "\n",
    "test_by_class = []\n",
    "for i in range(0,10):\n",
    "    inds = (y_test == i)\n",
    "    test_by_class.append(x_test[inds,:,:,:])\n",
    "    \n",
    "embs_by_class = []\n",
    "for val in test_by_class:\n",
    "    embs_by_class.append(base_net.predict(val, batch_size=100))\n",
    "\n",
    "plt.figure()\n",
    "colors = ['b', 'c', 'g', [0.3, 0.3, 0.3], 'm', 'r', 'y', [1., 1., 0.5], 'w', [0.25, 1.0, 0.25]]\n",
    "for i, embs in enumerate(embs_by_class[:]):\n",
    "    #embs = embs / np.expand_dims(np.linalg.norm(embs,axis=1),-1)\n",
    "    #inds = (embs[:,0] > -60.) * (embs[:,0] < 20) * (embs[:,1] < 40) * (embs[:,1] > -40)\n",
    "    plt.scatter(embs[:,0],embs[:,1], c=colors[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_random_neg_pairs(embs_by_class, num_pairs):\n",
    "    dists = np.zeros((num_pairs,))\n",
    "    for i in range(0,num_pairs):\n",
    "        class_ind_1 = np.random.randint(0,10)\n",
    "        class_ind_2 = rand.sample(range(0,class_ind_1)+range(class_ind_1+1,10), 1)[0]\n",
    "        sample1 = np.random.randint(0,embs_by_class[class_ind_1].shape[0])\n",
    "        sample2 = np.random.randint(0,embs_by_class[class_ind_2].shape[0])\n",
    "        \n",
    "        dists[i] = np.linalg.norm(embs_by_class[class_ind_1][sample1] - embs_by_class[class_ind_2][sample2])\n",
    "    return dists\n",
    "\n",
    "def get_random_pos_pairs(embs_by_class, num_pairs):\n",
    "    dists = np.zeros((num_pairs,))\n",
    "    for i in range(0,num_pairs):\n",
    "        class_ind = np.random.randint(0,10)\n",
    "        while True:\n",
    "            sample1 = np.random.randint(0,embs_by_class[class_ind].shape[0])\n",
    "            sample2 = np.random.randint(0,embs_by_class[class_ind].shape[0])\n",
    "            if sample1 is not sample2:\n",
    "                break\n",
    "        dists[i] = np.linalg.norm(embs_by_class[class_ind][sample1] - embs_by_class[class_ind][sample2])\n",
    "    return dists\n",
    "\n",
    "plt.figure()\n",
    "out = plt.hist(get_random_pos_pairs(embs_by_class, 5000), bins=100)\n",
    "out = plt.hist(get_random_neg_pairs(embs_by_class, 5000), bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity Softmax ([paper](https://arxiv.org/pdf/1704.06369.pdf))\n",
    "We do this in practice just by normalizing the feature vector and adding a constraint of row-wise normalization to the weight matrix in the softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = Input(input_size)\n",
    "base_net = get_feature_extractor()\n",
    "feature_vec = base_net(input_layer)\n",
    "\n",
    "# We're using 100 as the scale factor, which is certainly larger than the actual\n",
    "# required scale for 10 classes.\n",
    "feature_vec = Lambda(lambda x: 100. * K.l2_normalize(x,axis=0))(feature_vec)\n",
    "softmax_layer = Dense(num_classes, use_bias=False, activation='softmax', kernel_constraint=constraints.UnitNorm())\n",
    "prediction = softmax_layer(feature_vec)\n",
    "\n",
    "model = Model(input_layer, prediction)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change batch size to change \"tight-ness\" of clusters\n",
    "model.fit(x_train, to_categorical(y_train), epochs=10, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's check the scatter plot and distance distributions\n",
    "\n",
    "encoder = Model(input_layer, feature_vec)\n",
    "\n",
    "test_by_class = []\n",
    "for i in range(0,10):\n",
    "    inds = (y_test == i)\n",
    "    test_by_class.append(x_test[inds,:,:,:])\n",
    "    \n",
    "embs_by_class = []\n",
    "for val in test_by_class:\n",
    "    embs_by_class.append(base_net.predict(val, batch_size=100))\n",
    "\n",
    "plt.figure()\n",
    "colors = ['b', 'c', 'g', [0.3, 0.3, 0.3], 'm', 'r', 'y', [1., 1., 0.5], 'w', [0.25, 1.0, 0.25]]\n",
    "for i, embs in enumerate(embs_by_class[:]):\n",
    "    #embs = embs / np.expand_dims(np.linalg.norm(embs,axis=1),-1)\n",
    "    #inds = (embs[:,0] > -60.) * (embs[:,0] < 20) * (embs[:,1] < 40) * (embs[:,1] > -40)\n",
    "    plt.scatter(embs[:,0],embs[:,1], c=colors[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "out = plt.hist(get_random_pos_pairs(embs_by_class, 5000), bins=100)\n",
    "out = plt.hist(get_random_neg_pairs(embs_by_class, 5000), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
