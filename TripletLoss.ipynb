{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the batch-hard triplet loss in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the dimensionality of the feature vector, the number of classes randomly sampled per batch ('p' in the paper) and the number of samples randomly selected per class per batch ('k' in the paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, merge\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model, Sequential\n",
    "from keras import objectives\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras import callbacks\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rand\n",
    "import os\n",
    "from scipy.misc import imread, imsave, imresize\n",
    "from skimage import transform, filters\n",
    "from skimage import color\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "num_class_per_batch = 10\n",
    "num_samples_per_class = 10\n",
    "batch_size = num_class_per_batch*num_samples_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll load the Mnist data, preprocess, and make sampling batches easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "original_img_size = (28,28,1)\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape((x_train.shape[0],) + original_img_size)\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape((x_test.shape[0],) + original_img_size)\n",
    "train_by_class = []\n",
    "val_by_class = []\n",
    "for i in range(0,10):\n",
    "    inds = (y_train == i)\n",
    "    train_by_class.append(x_train[inds,:,:,:])\n",
    "    inds = (y_test == i)\n",
    "    val_by_class.append(x_test[inds,:,:,:])\n",
    "    \n",
    "def get_batch():\n",
    "    batch = np.zeros((batch_size, 28,28,1))\n",
    "    class_inds = rand.sample(range(0,10), num_class_per_batch)\n",
    "    labels = []\n",
    "    for i, cls_ind in enumerate(class_inds):\n",
    "        sample_inds = rand.sample(range(0,train_by_class[cls_ind].shape[0]), num_samples_per_class)\n",
    "        batch[i*num_samples_per_class:(i+1)*num_samples_per_class,:,:,:] = train_by_class[cls_ind][sample_inds,:,:,:]\n",
    "        labels += num_samples_per_class * [cls_ind]\n",
    "    return (batch, np.array(labels))\n",
    "\n",
    "def get_val_batch():\n",
    "    batch = np.zeros((batch_size, 28,28,1))\n",
    "    class_inds = rand.sample(range(0,10), num_class_per_batch)\n",
    "    labels = []\n",
    "    for i, cls_ind in enumerate(class_inds):\n",
    "        sample_inds = rand.sample(range(0,val_by_class[cls_ind].shape[0]), num_samples_per_class)\n",
    "        batch[i*num_samples_per_class:(i+1)*num_samples_per_class,:,:,:] = val_by_class[cls_ind][sample_inds,:,:,:]\n",
    "        labels += num_samples_per_class * [cls_ind]\n",
    "    return (batch, np.array(labels))\n",
    "\n",
    "def data_generator(with_labels=False):\n",
    "    while True:\n",
    "        batch, labels = get_batch()\n",
    "        if not with_labels:\n",
    "            yield (batch, np.zeros((num_class_per_batch*num_samples_per_class,)))\n",
    "        else:\n",
    "            yield(batch, [np.zeros((batch_size,)), to_categorical(labels, num_classes=10)])\n",
    "        \n",
    "def validation_generator(with_labels=False):\n",
    "    while True:\n",
    "        batch, labels = get_val_batch()\n",
    "        if not with_labels:\n",
    "            yield (batch, np.zeros((num_class_per_batch*num_samples_per_class,)))\n",
    "        else:\n",
    "            yield (batch, [np.zeros((batch_size,)), to_categorical(labels, num_classes=10)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_base_net():\n",
    "    net = Sequential()\n",
    "    net.add(Conv2D(32, (3,3), input_shape=(28,28,1), activation='relu', strides=(2,2), padding='same'))\n",
    "    net.add(BatchNormalization())\n",
    "    net.add(Conv2D(64, (3,3), activation='relu', strides=(2,2), padding='same'))\n",
    "    net.add(BatchNormalization())\n",
    "    net.add(Conv2D(128, (3,3), activation='relu', strides=(2,2), padding='valid'))\n",
    "    net.add(BatchNormalization())\n",
    "    net.add(Conv2D(128, (3,3), activation='relu', strides=(1,1), padding='same', name='conv4'))\n",
    "    net.add(BatchNormalization())\n",
    "    net.add(Flatten())\n",
    "    net.add(Dense(256, activation='relu'))\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_pairwise_dists(x, num_p=num_class_per_batch, num_k=num_samples_per_class, margin=0.5):\n",
    "    # pairwise distances for whole batch\n",
    "    # (redundant computation but probably still faster than alternative)\n",
    "    norms = tf.reduce_sum(x*x, 1)\n",
    "    norms = tf.reshape(norms, [-1, 1])\n",
    "    dists = norms - 2*tf.matmul(x, x, transpose_b=True) + tf.transpose(norms)\n",
    "    dists = K.sqrt(K.relu(dists))\n",
    "    \n",
    "    # We can experiment with using this instead of max\n",
    "    def log_sum_exp(xin, axis=1):\n",
    "        e_x = tf.reduce_sum(tf.exp(xin), axis)\n",
    "        log_sum = tf.log(e_x)\n",
    "        return log_sum\n",
    "    \n",
    "    # get the max intra-class distance for each sample\n",
    "    max_pos = [tf.reduce_max(tf.slice(dists, [i*num_k, i*num_k], [num_k, num_k]),axis=1) for i in range(0, num_p)]\n",
    "    max_pos = K.concatenate(max_pos, axis=0)\n",
    "    \n",
    "    # get the min inter-class distance for each sample\n",
    "    min_neg = []\n",
    "    for i in xrange(0,num_p):\n",
    "        left = tf.slice(dists, [i*num_k, 0], [num_k, i*num_k])\n",
    "        right = tf.slice(dists, [i*num_k, (i+1)*num_k], [num_k, (num_p-i-1)*num_k])\n",
    "        min_neg.append(tf.reduce_min(K.concatenate([left, right], axis=1), axis=1))\n",
    "    min_neg = K.concatenate(min_neg, axis=0)\n",
    "    \n",
    "    min_max = K.concatenate([K.expand_dims(max_pos, axis=-1), K.expand_dims(min_neg, axis=-1)], axis=1)\n",
    "    return min_max\n",
    "\n",
    "def get_triplet_dists(x, margin=0.5):\n",
    "    x = K.transpose(x)\n",
    "    max_pos = tf.gather(x, 0)\n",
    "    min_neg = tf.gather(x, 1)\n",
    "    # Use relu or softplus\n",
    "    L_triplet = K.expand_dims(K.softplus( margin + max_pos - min_neg ),1)\n",
    "    return L_triplet\n",
    "        \n",
    "def pairwise_shape(inshape):\n",
    "    return (inshape[0], 2)\n",
    "\n",
    "def triplet_shape(inshape):\n",
    "    return (inshape[0], 1)\n",
    "\n",
    "def triplet_loss(zeros, dists):\n",
    "    dists = batch_hard_triplets(dists)\n",
    "    return K.mean(dists, axis=0)\n",
    "\n",
    "inp = Input((28,28,1))\n",
    "base_net = get_base_net()\n",
    "base_out = base_net(inp)\n",
    "\n",
    "# We branch the net into the classification branch\n",
    "embeddings = Dense(latent_dim, activation=None)(base_out)\n",
    "class_output = Dense(10, use_bias=True, activation='softmax')(embeddings)\n",
    "\n",
    "# And now the triplet loss branch\n",
    "pairwise_dists = Lambda(get_pairwise_dists, output_shape=pairwise_shape)(embeddings)\n",
    "triplet_dists = Lambda(get_triplet_dists, output_shape=triplet_shape)(pairwise_dists)\n",
    "\n",
    "# Output the pairwise dists for logging\n",
    "triplet_net = Model(inp, outputs=[triplet_dists, pairwise_dists])\n",
    "classify_net = Model(inp, class_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is hacky for now. The loss is the mean absolute error with zeros as the labels.\n",
    "# I can use add_loss() on the triplet_dists, but I don't know if I can weight the loss\n",
    "# that way for when I add cross-entropy\n",
    "\n",
    "triplet_net.compile(loss=['mae', None], optimizer=Adam())\n",
    "classify_net.compile(loss='categorical_crossentropy', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre-train on Classification (however many epochs you like):\n",
    "# (Make sure, if you have an NVIDIA GPU, that you are using it. You will need to have built Tensorflow\n",
    "# with CUDA comptibility or have installed tensorflow-gpu from pip.)\n",
    "classify_net.fit(x_train, to_categorical(y_train), epochs=3)\n",
    "\n",
    "# This will output the classification net predictions on a batch of samples.\n",
    "# We expect to get rows of uniform numbers, since the batch formation is in globs of classes\n",
    "classify_net.predict(get_val_batch()[0], batch_size=100).argmax(axis=1).reshape([num_class_per_batch,num_samples_per_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This I use to log the pairwise distances for positive and negative pairs over the course of training\n",
    "\n",
    "max_pos_avg = []\n",
    "min_neg_avg = []\n",
    "class triplet_callback(callbacks.Callback):\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        pair_dists = self.model.predict(get_val_batch()[0], batch_size=batch_size)[1].mean(axis=0)\n",
    "        max_pos_avg.append(pair_dists[0])\n",
    "        min_neg_avg.append(pair_dists[1])\n",
    "        \n",
    "# Now let's train\n",
    "# Typically this won't work unless the latent dimension is at least 3.\n",
    "triplet_net.fit_generator(generator=data_generator(),\n",
    "    steps_per_epoch = 600,\n",
    "    epochs = 5,\n",
    "    validation_data=validation_generator(),\n",
    "    validation_steps=20, verbose=1,\n",
    "    callbacks=[triplet_callback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the pairwise distances seperated over the course of training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One line will plot the max positive pair distance (in a batch) at each iteration\n",
    "# Another line will plot the min negative pair distance (in a batch) at each iteration\n",
    "plt.figure()\n",
    "plt.plot(np.array(min_neg_avg[:]))\n",
    "plt.plot(np.array(max_pos_avg[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's call the base_net + embedding_layer the \"encoder\".\n",
    "encoder = Model(inp, embeddings)\n",
    "\n",
    "# Then we pass all the test set through the encoder, organized by class\n",
    "embs_by_class = []\n",
    "for val in val_by_class:\n",
    "    embs_by_class.append(encoder.predict(val, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = ['b', 'c', 'g', 'k', 'm', 'r', 'y', [1., 1., 0.5], 'w', [0.25, 1.0, 0.25]]\n",
    "plt.figure()\n",
    "for i, embs in enumerate(embs_by_class):\n",
    "    out = plt.scatter(embs[:,0], embs[:,1], c=colors[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the distance histograms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_random_neg_pairs(embs_by_class, num_pairs):\n",
    "    dists = np.zeros((num_pairs,))\n",
    "    for i in range(0,num_pairs):\n",
    "        class_ind_1 = np.random.randint(0,10)\n",
    "        class_ind_2 = rand.sample(range(0,class_ind_1)+range(class_ind_1+1,10), 1)[0]\n",
    "        sample1 = np.random.randint(0,embs_by_class[class_ind_1].shape[0])\n",
    "        sample2 = np.random.randint(0,embs_by_class[class_ind_2].shape[0])\n",
    "        \n",
    "        dists[i] = np.linalg.norm(embs_by_class[class_ind_1][sample1] - embs_by_class[class_ind_2][sample2])\n",
    "    return dists\n",
    "\n",
    "def get_random_pos_pairs(embs_by_class, num_pairs):\n",
    "    dists = np.zeros((num_pairs,))\n",
    "    for i in range(0,num_pairs):\n",
    "        class_ind = np.random.randint(0,10)\n",
    "        while True:\n",
    "            sample1 = np.random.randint(0,embs_by_class[class_ind].shape[0])\n",
    "            sample2 = np.random.randint(0,embs_by_class[class_ind].shape[0])\n",
    "            if sample1 is not sample2:\n",
    "                break\n",
    "        dists[i] = np.linalg.norm(embs_by_class[class_ind][sample1] - embs_by_class[class_ind][sample2])\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "out = plt.hist(get_random_pos_pairs(embs_by_class, 5000), bins=100)\n",
    "out = plt.hist(get_random_neg_pairs(embs_by_class, 5000), bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can try to train simultaneously with classification (instead of just pre-training with classification.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "triplet_net = Model(inp, outputs=[triplet_dists, pairwise_dists, class_output])\n",
    "triplet_net.compile(loss=['mae', None, 'categorical_crossentropy'], loss_weights=[0.3, 0, 0.7], optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_pos_avg = []\n",
    "min_neg_avg = []\n",
    "\n",
    "triplet_net.fit_generator(generator=data_generator(with_labels=True),\n",
    "    steps_per_epoch = 600,\n",
    "    epochs = 5,\n",
    "    validation_data=validation_generator(with_labels=True),\n",
    "    validation_steps=20, verbose=1,\n",
    "    callbacks=[triplet_callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot everything again to compare:\n",
    "embs_by_class = []\n",
    "for val in val_by_class:\n",
    "    embs_by_class.append(encoder.predict(val, batch_size=batch_size))\n",
    "plt.figure()\n",
    "for i, embs in enumerate(embs_by_class):\n",
    "    out = plt.scatter(embs[:,0], embs[:,1], c=colors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "out = plt.hist(get_random_pos_pairs(embs_by_class, 5000), bins=100)\n",
    "out = plt.hist(get_random_neg_pairs(embs_by_class, 5000), bins=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
